{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8f0feb4f-8497-4d41-b400-8af1e4ad242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-End IR System\n",
    "# Elijah Hoedl\n",
    "# Assistance by Copilot\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from nltk.metrics import edit_distance\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8a9f3689-c1a9-47f9-b308-582d1ea8e777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CRAWLER\n",
    "class Crawler:\n",
    "    def __init__(self, base_url, max_pages=50, max_depth=2, save_dir='corpus'):\n",
    "        self.base_url = base_url\n",
    "        self.max_pages = max_pages\n",
    "        self.max_depth = max_depth\n",
    "        self.visited = set()\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.pages_saved = 0\n",
    "\n",
    "        # Clear corpus folder if it exists\n",
    "        if os.path.exists(save_dir):\n",
    "            shutil.rmtree(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.pages_saved = 0\n",
    "\n",
    "    def crawl_site(self, url=None, depth=0):\n",
    "        if url is None:\n",
    "            url = self.base_url\n",
    "        if depth > self.max_depth or self.pages_saved >= self.max_pages:\n",
    "            return\n",
    "        if url in self.visited:\n",
    "            return\n",
    "        self.visited.add(url)\n",
    "        try:\n",
    "            print(f\"Crawling {self.pages_saved + 1}/{self.max_pages}\")\n",
    "            r = requests.get(url, timeout=5)\n",
    "            html = r.text\n",
    "            # Save HTML\n",
    "            filename = os.path.join(self.save_dir, f\"page_{self.pages_saved}.html\")\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(html)\n",
    "            self.pages_saved += 1\n",
    "            # Find links\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for link_tag in soup.find_all('a', href=True):\n",
    "                link = urljoin(url, link_tag['href'])\n",
    "                if urlparse(link).netloc == urlparse(self.base_url).netloc:\n",
    "                    self.crawl_site(link, depth + 1)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to crawl {url}: {e}\")\n",
    "\n",
    "    def crawl(self):\n",
    "        self.crawl_site(self.base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9111d41f-f278-4f9d-b231-2ef7f54f7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEXER\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self, corpus_dir='corpus', index_file='index.json'):\n",
    "        self.corpus_dir = corpus_dir\n",
    "        self.index_file = index_file\n",
    "        self.documents = []\n",
    "        self.doc_ids = []\n",
    "\n",
    "    def preprocess_html(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        text = soup.get_text(separator=' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return(text.lower())\n",
    "\n",
    "    def build_index(self):\n",
    "        # Load documents\n",
    "        for filename in os.listdir(self.corpus_dir):\n",
    "            if filename.endswith(\".html\"):\n",
    "                path = os.path.join(self.corpus_dir, filename)\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    html = f.read()\n",
    "                    text = self.preprocess_html(html)\n",
    "                    self.documents.append(text)\n",
    "                    self.doc_ids.append(filename)\n",
    "\n",
    "        # Build TF-IDF\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)\n",
    "\n",
    "        # Save index as JSON\n",
    "        index_data = {\n",
    "            \"doc_ids\": self.doc_ids,\n",
    "            \"vocabulary\": self.vectorizer.get_feature_names_out().tolist(),\n",
    "            \"tfidf_matrix\": self.tfidf_matrix.toarray().tolist()\n",
    "        }\n",
    "        with open(self.index_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(index_data, f)\n",
    "        print(f\"Index saved to {self.index_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c84a6077-3da5-4b12-b320-d3faade8300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY PROCESSOR\n",
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, index_file='index.json'):\n",
    "        self.index_file = index_file\n",
    "        self.load_index()\n",
    "\n",
    "    def load_index(self):\n",
    "        import json\n",
    "        with open(self.index_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        self.doc_ids = data['doc_ids']\n",
    "        self.vocab = {term: i for i, term in enumerate(data['vocabulary'])}\n",
    "        self.tfidf_matrix = np.array(data['tfidf_matrix'])\n",
    "\n",
    "    def preprocess_query(self, query):\n",
    "        return(re.sub(r'\\s+', ' ', query.lower()))\n",
    "\n",
    "    def correct_spelling(self, word):\n",
    "        # Return the closest vocabulary term if word not found.\n",
    "        if word in self.vocab:\n",
    "            return word\n",
    "        min_distance = float(\"inf\")\n",
    "        closest_term = None\n",
    "        for term in self.vocab.keys():\n",
    "            dist = edit_distance(word, term)\n",
    "            if dist < min_distance:\n",
    "                min_distance = dist\n",
    "                closest_term = term\n",
    "        if closest_term:\n",
    "            print(f\"Did you mean: {closest_term}? Using corrected term.\")\n",
    "            return closest_term\n",
    "        return(word)\n",
    "\n",
    "    def query_to_vector(self, query):\n",
    "        # Convert query string to a TF-IDF vector compatible with indexed docs.\n",
    "        query_vec = np.zeros(len(self.vocab))\n",
    "        query_words = self.preprocess_query(query).split()\n",
    "        for word in query_words:\n",
    "            corrected = self.correct_spelling(word)\n",
    "            if corrected in self.vocab:\n",
    "                query_vec[self.vocab[corrected]] = 1\n",
    "        return(query_vec.reshape(1, -1))\n",
    "\n",
    "    def search(self, query, top_k=5):\n",
    "        query_vec = self.query_to_vector(query)\n",
    "        scores = cosine_similarity(query_vec, self.tfidf_matrix)[0]\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        results = [{\"doc_id\": self.doc_ids[i], \"score\": float(scores[i])} for i in top_indices]\n",
    "        return(results)\n",
    "\n",
    "\n",
    "def save_query_results_csv(queries, output_csv='query_results.csv', top_k=5):\n",
    "    qp = QueryProcessor(index_file='index.json')\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['query', 'doc_id', 'score'])\n",
    "        for q in queries:\n",
    "            results = qp.search(q, top_k=top_k)\n",
    "            for r in results:\n",
    "                writer.writerow([q, r['doc_id'], r['score']])\n",
    "    print(f\"Query results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "02daaf8d-344d-4fed-a50b-6f9868b56222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 1/10\n",
      "Crawling 2/10\n",
      "Crawling 3/10\n",
      "Crawling 4/10\n",
      "Crawling 5/10\n",
      "Crawling 6/10\n",
      "Crawling 7/10\n",
      "Crawling 8/10\n",
      "Crawling 9/10\n",
      "Crawling 10/10\n",
      "Index saved to index.json\n",
      "Did you mean: historical? Using corrected term.\n",
      "Query results saved to query_results.csv\n"
     ]
    }
   ],
   "source": [
    "site = \"https://books.toscrape.com/\"\n",
    "crawler = Crawler(site, max_pages=10, max_depth=2)\n",
    "crawler.crawl()\n",
    "\n",
    "indexer = Indexer()\n",
    "indexer.build_index()\n",
    "\n",
    "queries = [\"Histrorical\"]\n",
    "save_query_results_csv(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8cf3b8f8-13c5-4e62-a370-7ad5b1c6e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "query_processor = QueryProcessor(index_file='index.json')\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def search_api():\n",
    "    data = request.get_json()\n",
    "    query = data.get(\"query\", \"\")\n",
    "    top_k = data.get(\"top_k\", 5)\n",
    "    results = query_processor.search(query, top_k=top_k)\n",
    "    return jsonify(results)\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65d30b-3c7b-4cac-9a26-9f645b7b68af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " histrorical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Dec/2025 14:35:18] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you mean: historical? Using corrected term.\n",
      "[{'doc_id': 'page_5.html', 'score': 0.08752523499162378}, {'doc_id': 'page_8.html', 'score': 0.05848660342984474}, {'doc_id': 'page_3.html', 'score': 0.05601379570814349}]\n",
      "Continue? (Y/N)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " historical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Dec/2025 14:35:28] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 'page_5.html', 'score': 0.08752523499162378}, {'doc_id': 'page_8.html', 'score': 0.05848660342984474}, {'doc_id': 'page_3.html', 'score': 0.05601379570814349}]\n",
      "Continue? (Y/N)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Philosophy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Dec/2025 14:35:48] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 'page_8.html', 'score': 0.11697320685968948}, {'doc_id': 'page_3.html', 'score': 0.028006897854071747}, {'doc_id': 'page_7.html', 'score': 0.018997906134360466}]\n",
      "Continue? (Y/N)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hpilosohpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Dec/2025 14:36:05] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you mean: philosophy? Using corrected term.\n",
      "[{'doc_id': 'page_8.html', 'score': 0.11697320685968948}, {'doc_id': 'page_3.html', 'score': 0.028006897854071747}, {'doc_id': 'page_7.html', 'score': 0.018997906134360466}]\n",
      "Continue? (Y/N)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " WRONGINPUT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input.\n",
      "Continue? (Y/N)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " black\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Dec/2025 14:36:59] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 'page_9.html', 'score': 0.06281930262283093}, {'doc_id': 'page_2.html', 'score': 0.03324204103288293}, {'doc_id': 'page_1.html', 'score': 0.03285743060565798}]\n",
      "Continue? (Y/N)\n"
     ]
    }
   ],
   "source": [
    "stop = False\n",
    "while not stop:\n",
    "    print(\"Enter Query:\")\n",
    "    query = input()\n",
    "    response = requests.post(\n",
    "        \"http://localhost:5000/search\",\n",
    "        json={\"query\": query, \"top_k\": 3}\n",
    "    )\n",
    "    print(response.json())\n",
    "    print(\"Continue? (Y/N)\")\n",
    "    cont = input()\n",
    "    while not stop:\n",
    "        if cont == \"Y\":\n",
    "            stop = False\n",
    "            break\n",
    "        if cont == \"N\":\n",
    "            stop = True\n",
    "            print(\"Thanks for searching!\")\n",
    "        else:\n",
    "            print(\"Invalid input.\")\n",
    "            print(\"Continue? (Y/N)\")\n",
    "            cont = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5daed46-f0a8-4afe-8064-4ae38d2e4ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
