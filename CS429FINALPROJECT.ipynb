{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8f0feb4f-8497-4d41-b400-8af1e4ad242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-End IR System\n",
    "# Elijah Hoedl\n",
    "# Assistance using Copilot\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from nltk.metrics import edit_distance\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8a9f3689-c1a9-47f9-b308-582d1ea8e777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CRAWLER\n",
    "class Crawler:\n",
    "    def __init__(self, base_url, max_pages=50, max_depth=2, save_dir='corpus'):\n",
    "        self.base_url = base_url\n",
    "        self.max_pages = max_pages\n",
    "        self.max_depth = max_depth\n",
    "        self.visited = set()\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.pages_saved = 0\n",
    "\n",
    "        # Clear corpus folder if it exists\n",
    "        if os.path.exists(save_dir):\n",
    "            shutil.rmtree(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        self.pages_saved = 0\n",
    "\n",
    "    def crawl_site(self, url=None, depth=0):\n",
    "        # Crawl Sites\n",
    "        if url is None:\n",
    "            url = self.base_url\n",
    "        if depth > self.max_depth or self.pages_saved >= self.max_pages:\n",
    "            return\n",
    "        if url in self.visited:\n",
    "            return\n",
    "        self.visited.add(url)\n",
    "        try:\n",
    "            print(f\"Crawling {self.pages_saved + 1}/{self.max_pages}\")\n",
    "            r = requests.get(url, timeout=5)\n",
    "            html = r.text\n",
    "            # Save html file\n",
    "            filename = os.path.join(self.save_dir, f\"page_{self.pages_saved}.html\")\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(html)\n",
    "            self.pages_saved += 1\n",
    "            # Find links\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for link_tag in soup.find_all('a', href=True):\n",
    "                link = urljoin(url, link_tag['href'])\n",
    "                if urlparse(link).netloc == urlparse(self.base_url).netloc:\n",
    "                    self.crawl_site(link, depth + 1)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to crawl {url}: {e}\")\n",
    "\n",
    "    def crawl(self):\n",
    "        self.crawl_site(self.base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9111d41f-f278-4f9d-b231-2ef7f54f7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEXER\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self, corpus_dir='corpus', index_file='index.json'):\n",
    "        self.corpus_dir = corpus_dir\n",
    "        self.index_file = index_file\n",
    "        self.documents = []\n",
    "        self.doc_ids = []\n",
    "\n",
    "    def preprocess_html(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        text = soup.get_text(separator=' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return(text.lower())\n",
    "\n",
    "    def build_index(self):\n",
    "        # Load documents from corpus\n",
    "        for filename in os.listdir(self.corpus_dir):\n",
    "            if filename.endswith(\".html\"):\n",
    "                path = os.path.join(self.corpus_dir, filename)\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    html = f.read()\n",
    "                    text = self.preprocess_html(html)\n",
    "                    self.documents.append(text)\n",
    "                    self.doc_ids.append(filename)\n",
    "\n",
    "        # Build TF-IDF matrix\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)\n",
    "\n",
    "        # Save index as json\n",
    "        index_data = {\n",
    "            \"doc_ids\": self.doc_ids,\n",
    "            \"vocabulary\": self.vectorizer.get_feature_names_out().tolist(),\n",
    "            \"tfidf_matrix\": self.tfidf_matrix.toarray().tolist()\n",
    "        }\n",
    "        with open(self.index_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(index_data, f)\n",
    "        print(f\"Index saved to {self.index_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c84a6077-3da5-4b12-b320-d3faade8300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY PROCESSOR\n",
    "\n",
    "class QueryProcessor:\n",
    "    def __init__(self, index_file='index.json'):\n",
    "        self.index_file = index_file\n",
    "        self.load_index()\n",
    "\n",
    "    def load_index(self):\n",
    "        # Load index\n",
    "        import json\n",
    "        with open(self.index_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        self.doc_ids = data['doc_ids']\n",
    "        self.vocab = {term: i for i, term in enumerate(data['vocabulary'])}\n",
    "        self.tfidf_matrix = np.array(data['tfidf_matrix'])\n",
    "\n",
    "    def preprocess_query(self, query):\n",
    "        # Make query lowercase\n",
    "        return(re.sub(r'\\s+', ' ', query.lower()))\n",
    "\n",
    "    def correct_spelling(self, word):\n",
    "        # Return the closest vocabulary term if word not found\n",
    "        if word in self.vocab:\n",
    "            return word\n",
    "        min_distance = float(\"inf\")\n",
    "        closest_term = None\n",
    "        for term in self.vocab.keys():\n",
    "            dist = edit_distance(word, term)\n",
    "            if dist < min_distance:\n",
    "                min_distance = dist\n",
    "                closest_term = term\n",
    "        if closest_term:\n",
    "            print(f\"Did you mean: {closest_term}? Using corrected term.\")\n",
    "            return closest_term\n",
    "        return(word)\n",
    "\n",
    "    def query_to_vector(self, query):\n",
    "        # Convert query string to a TF-IDF vector compatible with indexed docs\n",
    "        query_vec = np.zeros(len(self.vocab))\n",
    "        query_words = self.preprocess_query(query).split()\n",
    "        for word in query_words:\n",
    "            corrected = self.correct_spelling(word)\n",
    "            if corrected in self.vocab:\n",
    "                query_vec[self.vocab[corrected]] = 1\n",
    "        return(query_vec.reshape(1, -1))\n",
    "\n",
    "    def search(self, query, top_k=5):\n",
    "        # Search index\n",
    "        query_vec = self.query_to_vector(query)\n",
    "        scores = cosine_similarity(query_vec, self.tfidf_matrix)[0]\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        results = [{\"doc_id\": self.doc_ids[i], \"score\": float(scores[i])} for i in top_indices]\n",
    "        return(results)\n",
    "\n",
    "\n",
    "def save_query_results_csv(queries, output_csv='query_results.csv', top_k=5):\n",
    "    # Save query results to csv file\n",
    "    qp = QueryProcessor(index_file='index.json')\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['query', 'doc_id', 'score'])\n",
    "        for q in queries:\n",
    "            results = qp.search(q, top_k=top_k)\n",
    "            for r in results:\n",
    "                writer.writerow([q, r['doc_id'], r['score']])\n",
    "    print(f\"Query results saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "02daaf8d-344d-4fed-a50b-6f9868b56222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling 1/100\n",
      "Crawling 2/100\n",
      "Crawling 3/100\n",
      "Crawling 4/100\n",
      "Crawling 5/100\n",
      "Crawling 6/100\n",
      "Crawling 7/100\n",
      "Crawling 8/100\n",
      "Crawling 9/100\n",
      "Crawling 10/100\n",
      "Crawling 11/100\n",
      "Crawling 12/100\n",
      "Crawling 13/100\n",
      "Crawling 14/100\n",
      "Crawling 15/100\n",
      "Crawling 16/100\n",
      "Crawling 17/100\n",
      "Crawling 18/100\n",
      "Crawling 19/100\n",
      "Crawling 20/100\n",
      "Crawling 21/100\n",
      "Crawling 22/100\n",
      "Crawling 23/100\n",
      "Crawling 24/100\n",
      "Crawling 25/100\n",
      "Crawling 26/100\n",
      "Crawling 27/100\n",
      "Crawling 28/100\n",
      "Crawling 29/100\n",
      "Crawling 30/100\n",
      "Crawling 31/100\n",
      "Crawling 32/100\n",
      "Crawling 33/100\n",
      "Crawling 34/100\n",
      "Crawling 35/100\n",
      "Crawling 36/100\n",
      "Crawling 37/100\n",
      "Crawling 38/100\n",
      "Crawling 39/100\n",
      "Crawling 40/100\n",
      "Crawling 41/100\n",
      "Crawling 42/100\n",
      "Crawling 43/100\n",
      "Crawling 44/100\n",
      "Crawling 45/100\n",
      "Crawling 46/100\n",
      "Crawling 47/100\n",
      "Crawling 48/100\n",
      "Crawling 49/100\n",
      "Crawling 50/100\n",
      "Crawling 51/100\n",
      "Crawling 52/100\n",
      "Crawling 53/100\n",
      "Crawling 54/100\n",
      "Crawling 55/100\n",
      "Crawling 56/100\n",
      "Crawling 57/100\n",
      "Crawling 58/100\n",
      "Crawling 59/100\n",
      "Crawling 60/100\n",
      "Crawling 61/100\n",
      "Crawling 62/100\n",
      "Crawling 63/100\n",
      "Crawling 64/100\n",
      "Crawling 65/100\n",
      "Crawling 66/100\n",
      "Crawling 67/100\n",
      "Crawling 68/100\n",
      "Crawling 69/100\n",
      "Crawling 70/100\n",
      "Crawling 71/100\n",
      "Crawling 72/100\n",
      "Crawling 73/100\n",
      "Crawling 74/100\n",
      "Crawling 75/100\n",
      "Crawling 76/100\n",
      "Crawling 77/100\n",
      "Crawling 78/100\n",
      "Crawling 79/100\n",
      "Crawling 80/100\n",
      "Crawling 81/100\n",
      "Crawling 82/100\n",
      "Crawling 83/100\n",
      "Crawling 84/100\n",
      "Crawling 85/100\n",
      "Crawling 86/100\n",
      "Crawling 87/100\n",
      "Crawling 88/100\n",
      "Crawling 89/100\n",
      "Crawling 90/100\n",
      "Crawling 91/100\n",
      "Crawling 92/100\n",
      "Crawling 93/100\n",
      "Crawling 94/100\n",
      "Crawling 95/100\n",
      "Crawling 96/100\n",
      "Crawling 97/100\n",
      "Index saved to index.json\n",
      "Did you mean: historical? Using corrected term.\n",
      "Query results saved to query_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "site = \"https://books.toscrape.com/\"\n",
    "crawler = Crawler(site, max_pages=100, max_depth=3)\n",
    "crawler.crawl()\n",
    "\n",
    "indexer = Indexer()\n",
    "indexer.build_index()\n",
    "\n",
    "query = [\"Histrorical\"]\n",
    "save_query_results_csv(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8cf3b8f8-13c5-4e62-a370-7ad5b1c6e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "query_processor = QueryProcessor(index_file='index.json')\n",
    "\n",
    "@app.route('/search', methods=['POST'])\n",
    "def search_api():\n",
    "    data = request.get_json()\n",
    "    query = data.get(\"query\", \"\")\n",
    "    top_k = data.get(\"top_k\", 5)\n",
    "    results = query_processor.search(query, top_k=top_k)\n",
    "    return jsonify(results)\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65d30b-3c7b-4cac-9a26-9f645b7b68af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Classics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Classics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Dec/2025 13:04:36] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 'page_7.html', 'score': 0.10028295809075963}, {'doc_id': 'page_51.html', 'score': 0.07590549164043693}, {'doc_id': 'page_45.html', 'score': 0.07500404172475594}]\n",
      "Continue? (Y/N)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " How Music Works\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for How Music Works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Dec/2025 13:04:56] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 'page_76.html', 'score': 0.43415582460022184}, {'doc_id': 'page_15.html', 'score': 0.17107071597132362}, {'doc_id': 'page_42.html', 'score': 0.16455099356734287}]\n",
      "Continue? (Y/N)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Mark Fallon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Mark Fallon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Dec/2025 13:05:39] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 'page_87.html', 'score': 0.16635879417174554}, {'doc_id': 'page_60.html', 'score': 0.018952432426864587}, {'doc_id': 'page_96.html', 'score': 0.0}]\n",
      "Continue? (Y/N)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Philsophosy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Philsophosy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Dec/2025 13:06:16] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you mean: philosophy? Using corrected term.\n",
      "[{'doc_id': 'page_8.html', 'score': 0.14452292660839583}, {'doc_id': 'page_51.html', 'score': 0.07505918199458798}, {'doc_id': 'page_45.html', 'score': 0.07416778281097397}]\n",
      "Continue? (Y/N)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Query:\n"
     ]
    }
   ],
   "source": [
    "stop = False\n",
    "while not stop:\n",
    "    print(\"Enter Query:\")\n",
    "    query = input()\n",
    "    print(f\"Searching for {query}\")\n",
    "    response = requests.post(\n",
    "        \"http://localhost:5000/search\",\n",
    "        json={\"query\": query, \"top_k\": 3}\n",
    "    )\n",
    "    print(response.json())\n",
    "    print(\"Continue? (Y/N)\")\n",
    "    cont = input()\n",
    "    while not stop:\n",
    "        if cont == \"Y\":\n",
    "            stop = False\n",
    "            break\n",
    "        if cont == \"N\":\n",
    "            stop = True\n",
    "            print(\"Thanks for searching!\")\n",
    "        else:\n",
    "            print(\"Invalid input.\")\n",
    "            print(\"Continue? (Y/N)\")\n",
    "            cont = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5daed46-f0a8-4afe-8064-4ae38d2e4ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a566b-ed2e-4046-bf34-9d5c34ebce32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed5e61-df36-4043-b962-0e8f7eaf04e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0e9e5-403c-44eb-bfe3-612ab7356d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f4730-4274-4dfa-ba3b-4182b6137533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541da3f-e532-4ed0-8027-c2874a41f41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
